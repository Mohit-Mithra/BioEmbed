{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Runs_logreg.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "I7_rTF_S1IDc"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')\n",
        "!pwd\n",
        "%cd gdrive/My\\ Drive/IITM_internship"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lvm2E7Nc1UBr"
      },
      "source": [
        "import json\n",
        "import csv\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import random\n",
        "import math\n",
        "import operator\n",
        "import itertools\n",
        "import pickle\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib import pyplot\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.model_selection import StratifiedShuffleSplit\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from imblearn.over_sampling import SMOTE\n",
        "from imblearn.combine import SMOTETomek\n",
        "from collections import Counter\n",
        "from sklearn.metrics import cohen_kappa_score\n",
        "from sklearn.metrics import roc_curve\n",
        "from sklearn.metrics import roc_auc_score\n",
        "from sklearn.metrics import auc\n",
        "from sklearn.metrics import precision_recall_curve\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from joblib import Parallel, delayed\n",
        "from statistics import mean\n",
        "from sklearn.linear_model import LogisticRegression"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EKz9d2kn1v4G"
      },
      "source": [
        "#a dict containing the hormone and the list of genes associated with it in HGv1 database\n",
        "with open('./BioEmbedS/dataset/hgv1_hormone_genes.json') as json_file:\n",
        "    hormone_genes = json.load(json_file)\n",
        "    \n",
        "# This is a list of genes that are associated with multiple hormones\n",
        "dup_genes = []\n",
        "with open('./BioEmbedS/dataset/genes_associated_with_multiple_hormones.txt','r') as f:\n",
        "    for line in f:\n",
        "        dup_genes.append(line[:-1])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ab9vJ4Hw16Cs"
      },
      "source": [
        "with open('./BioEmbedS_TS/dataset/fasttext_word_embeddings.json') as json_file:\n",
        "    word_embeddings = json.load(json_file)\n",
        "\n",
        "print(len(word_embeddings))\n",
        "word_embeddings.keys()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3d_CWAnr243O"
      },
      "source": [
        "# get embedding for hormones having aliases by add the original and alias embedding\n",
        "alias_embeddings = dict()\n",
        "for hormone in hormone_genes.keys():\n",
        "    if \"/\" in hormone:\n",
        "        parts = hormone.split(\"/\")\n",
        "        w1 = word_embeddings[parts[0]]\n",
        "        w2 = word_embeddings[parts[1]]\n",
        "        alias_embeddings[hormone] = np.add(w1,w2)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cNXU7C8G3FCI"
      },
      "source": [
        "def get_xgb_estimator_results(X_train,y_train, X_val, y_val, params):\n",
        "\n",
        "    param = {}\n",
        "\n",
        "    param['learning_rate'] = params[0]\n",
        "    param['max_depth'] = params[1]\n",
        "    param['max_features'] = params[2]\n",
        "    xgb_model = xgb.XGBClassifier()\n",
        "    xgb_model.set_params(**param)\n",
        "\n",
        "    xgb_model.fit(X_train, y_train)\n",
        "    y_pred = xgb_model.predict(X_val)\n",
        "    score = cohen_kappa_score(y_val,y_pred)\n",
        "    param_str = \"xgb/\"+str(params[0])+\"/\"+str(params[1])+\"/\"+str(params[2])\n",
        "    return (param_str, score)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HZiG6YFB3xpA"
      },
      "source": [
        "# Apply SMOTE-Tomek links oversampling+undersampling methodology to get a balanced dataset (Same #genes associated with every hormone)\n",
        "# Takes input a dictionary with hormones and list of genes associated with them and the list of genes associated with multiple hormones.\n",
        "# Returns the trainig sets and a dict indicating which hormones are present in the training sets\n",
        "def get_oversampled_train_data(train_data,dup_genes):\n",
        "    train_marked = dict()\n",
        "    hor_map = dict()\n",
        "    X_train_smote = []\n",
        "    y_train_smote = []\n",
        "    eligible_genes = dict()\n",
        "    duplicate_genes = dict()\n",
        "    cnt = 1\n",
        "    # get the list of genes for each hormone after removing genes which are associated with multiple hormones (eligible genes).\n",
        "    for hormone in train_data.keys():\n",
        "        eligible_genes[hormone] = []\n",
        "        duplicate_genes[hormone] = []\n",
        "        for gene in train_data[hormone]:\n",
        "            if gene in dup_genes:\n",
        "                duplicate_genes[hormone].append(gene)\n",
        "            else:\n",
        "                eligible_genes[hormone].append(gene)\n",
        "        \n",
        "        # Consider a hormone only if it has atleast 3 eligible genes (constraint to apply SMOTE) and mark this gene.\n",
        "        if len(eligible_genes[hormone]) >= 3:\n",
        "            hor_map[cnt] = hormone\n",
        "            train_marked[hormone] = 1\n",
        "            for gene in eligible_genes[hormone]:\n",
        "                X_train_smote.append(word_embeddings[gene])\n",
        "                y_train_smote.append(cnt)\n",
        "            cnt += 1\n",
        "        else:\n",
        "            train_marked[hormone] = 0\n",
        "\n",
        "    # tranform dataset using smote-tomek\n",
        "    smote_strategy = SMOTETomek(random_state=42, smote=SMOTE(k_neighbors=2))\n",
        "    X_dataset_src_oversampled, y_dataset_src_oversampled = smote_strategy.fit_resample(np.array(X_train_smote),np.array(y_train_smote))\n",
        "    #counter = Counter(y_dataset_src_oversampled)\n",
        "    #print(counter)\n",
        "    \n",
        "    oversampled_genes_pos = dict()\n",
        "    oversampled_genes_neg = dict()\n",
        "    X_train_pos = []\n",
        "    # get the positive oversampled train data\n",
        "    for hormone, embedding in zip(y_dataset_src_oversampled, X_dataset_src_oversampled):\n",
        "        if hor_map[hormone] in oversampled_genes_pos.keys():\n",
        "            oversampled_genes_pos[hor_map[hormone]].append(embedding)   \n",
        "        else:\n",
        "            oversampled_genes_pos[hor_map[hormone]] = [embedding]\n",
        "        if \"/\" in hor_map[hormone]:\n",
        "            w1 = alias_embeddings[hor_map[hormone]]\n",
        "        else:\n",
        "            w1 = word_embeddings[hor_map[hormone]]\n",
        "        X_train_pos.append(np.concatenate([w1,embedding]))\n",
        "\n",
        "    # add back the genes associated with multiple hormones that were removed earlier\n",
        "    for hor in oversampled_genes_pos.keys():\n",
        "        if \"/\" in hor:\n",
        "            w1 = alias_embeddings[hor]\n",
        "        else:\n",
        "            w1 = word_embeddings[hor]\n",
        "        for gene in duplicate_genes[hor]:\n",
        "            w2 = word_embeddings[gene]\n",
        "            X_train_pos.append(np.concatenate([w1,w2]))\n",
        "\n",
        "    #get negative train data, randomly sample from oversamples embeddings known to be not associated with a hormone\n",
        "    X_train_neg = []\n",
        "    for hormone in oversampled_genes_pos.keys():\n",
        "        if hormone not in oversampled_genes_neg.keys():\n",
        "            oversampled_genes_neg[hormone] = []\n",
        "\n",
        "        cnt = len(oversampled_genes_pos[hormone]) + len(duplicate_genes[hormone])\n",
        "        if \"/\" in hormone:\n",
        "            w1 = alias_embeddings[hormone]\n",
        "        else:\n",
        "            w1 = word_embeddings[hormone]\n",
        "        rem_genes_embed = []\n",
        "        for hor in oversampled_genes_pos.keys():\n",
        "            if hor != hormone:\n",
        "                for embed in oversampled_genes_pos[hor]:\n",
        "                    rem_genes_embed.append(embed)\n",
        "                    \n",
        "        random.seed(42)\n",
        "        for embedding in random.sample(rem_genes_embed, cnt):\n",
        "            oversampled_genes_neg[hormone].append(embedding)\n",
        "            X_train_neg.append(list(w1)+list(embedding))\n",
        "\n",
        "    X_train_pos = np.array(X_train_pos)\n",
        "    X_train_neg = np.array(X_train_neg)\n",
        "    X_train = np.concatenate([X_train_pos, X_train_neg])\n",
        "\n",
        "    y_train_pos = np.ones((X_train_pos.shape[0],), dtype=int)\n",
        "    y_train_neg = np.zeros((X_train_neg.shape[0],), dtype=int)\n",
        "    y_train = np.concatenate([y_train_pos,y_train_neg])\n",
        "    \n",
        "    print(\"train shape\")\n",
        "    #print(X_train_pos.shape)\n",
        "    #print(X_train_neg.shape)\n",
        "    print(X_train.shape)\n",
        "    print(y_train.shape)\n",
        "    return X_train, y_train, train_marked"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sLUg9lX14fJg"
      },
      "source": [
        "# function that takes and input as a dict containing hormones and its associated list of genes and gets the \n",
        "# corresponding word vectors for each of these tokens and returns the set. Only hormomes used in training are considered\n",
        "def transform_X_values(data_dict,train_marked):\n",
        "    embeddings = []\n",
        "    for hormone in data_dict.keys():\n",
        "        if train_marked[hormone] == 1:\n",
        "            if \"/\" in hormone:\n",
        "                np1 = alias_embeddings[hormone]\n",
        "            else:\n",
        "                np1 = word_embeddings[hormone]\n",
        "            for gene in data_dict[hormone]:\n",
        "                np2 = word_embeddings[gene]\n",
        "                embeddings.append(np.concatenate([np1,np2]))\n",
        "    return np.array(embeddings)\n",
        "\n",
        "# same functionality as the above function but also filters genes based on the bins they are assigned to\n",
        "def transform_X_values_new(data_dict, bins, train_marked):\n",
        "    embeddings = []\n",
        "    for hormone in data_dict.keys():\n",
        "        if hormone in bins: \n",
        "            if train_marked[hormone] == 1:\n",
        "                if \"/\" in hormone:\n",
        "                    np1 = alias_embeddings[hormone]\n",
        "                else:\n",
        "                    np1 = word_embeddings[hormone]\n",
        "                for gene in data_dict[hormone]:\n",
        "                    np2 = word_embeddings[gene]\n",
        "                    embeddings.append(np.concatenate([np1,np2]))\n",
        "    return np.array(embeddings)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lM4h7w8M4wK7"
      },
      "source": [
        "# function that divides the hormones into 4 bins depending on the #genes associated. \n",
        "# Prints the results for each bins considering the predictions for hormones that are assigned to that bin.\n",
        "def get_binned_results(test_data, neg_test_data, train_marked, svclassifier):\n",
        "    gene_cnt = []\n",
        "    for hormone in test_data.keys():\n",
        "        if train_marked[hormone] == 1:\n",
        "            length = len(test_data[hormone])\n",
        "            gene_cnt.append(length)\n",
        "        \n",
        "    df = pd.DataFrame(gene_cnt)\n",
        "    res,bin_edges = pd.qcut(df[0], q=4,retbins=True)\n",
        "    bin1 = []\n",
        "    bin2 = []\n",
        "    bin3 = []\n",
        "    bin4 = []\n",
        "     \n",
        "    # divide hormones into bins\n",
        "    for hormone in test_data.keys():\n",
        "        length = len(test_data[hormone])\n",
        "        if length <= int(bin_edges[2]):\n",
        "            bin1.append(hormone)\n",
        "        elif length > int(bin_edges[2]) and length<= int(bin_edges[3]):\n",
        "            bin2.append(hormone)\n",
        "        elif length > int(bin_edges[3]) and length< int(bin_edges[4]):\n",
        "            bin3.append(hormone)\n",
        "        else:\n",
        "            bin4.append(hormone)\n",
        "\n",
        "    all_bins = []\n",
        "    all_bins.append(bin1)\n",
        "    all_bins.append(bin2)\n",
        "    all_bins.append(bin3)\n",
        "    all_bins.append(bin4)\n",
        "    \n",
        "    # get bin wise results\n",
        "    for i, bin_name in zip(range(4),all_bins):\n",
        "        print(\"Testing results for bin \"+str(i+1))\n",
        "        X_test_pos = transform_X_values_new(test_data, bin_name,train_marked)\n",
        "        X_test_neg = transform_X_values_new(neg_test_data, bin_name,train_marked)\n",
        "        X_test = np.concatenate([X_test_pos,X_test_neg])\n",
        "        y_test_pos = np.ones((X_test_pos.shape[0],), dtype=int)\n",
        "        y_test_neg = np.zeros((X_test_neg.shape[0],), dtype=int)\n",
        "        y_test = np.concatenate([y_test_pos, y_test_neg])\n",
        "        print(\"bin_size\")\n",
        "        print(X_test.shape)\n",
        "        y_pred_test = svclassifier.predict(X_test)\n",
        "        print(confusion_matrix(y_test, y_pred_test))\n",
        "        print(classification_report(y_test, y_pred_test))\n",
        "    \n",
        "# The genes associated with every hormone are divided into 5 bins. The below files contain the genes associated with a hormone\n",
        "# for each of the 5 bins in a dictionary form. Bins are also present for negatively associated genes.\n",
        "with open('./BioEmbedS/dataset/hgv1_hormone_gene_bins.json') as json_file:\n",
        "    hormone_gene_bins = json.load(json_file)\n",
        "    \n",
        "with open('./BioEmbedS/dataset/hgv1_negative_hormone_gene_bins.json') as json_file:\n",
        "    neg_hormone_gene_bins = json.load(json_file)\n",
        "\n",
        "# Set the range of parameters for xgb.\n",
        "param_str_set = []\n",
        "learning_rate = np.logspace(0.03, 0.3, num = 10) # default 0.1 \n",
        "max_depth = np.linspace(2, 6, num = 5, dtype=int) # default 3\n",
        "n_estimators = np.linspace(100,150, num = 5, dtype=int) # default 100\n",
        "\n",
        "\n",
        "    \n",
        "param_str_set = []\n",
        "rbf_param_list = []\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XHUayq9842nm"
      },
      "source": [
        "param_str_set = []\n",
        "rbf_param_list = []\n",
        "\n",
        "# # Set the range of parameters for SVM classifier\n",
        "# C_range = np.logspace(-4, 4, 9)\n",
        "# poly_C_range = np.logspace(-4, 0, 5)\n",
        "# gamma_range = np.logspace(-9, 2, 12)\n",
        "# degree_range = [2, 3, 5, 7]\n",
        "# for C in C_range:\n",
        "#     for gamma in gamma_range:\n",
        "#         rbf_param_list.append(('rbf',C,gamma))\n",
        "#         param_str_set.append(\"svm/rbf/\"+str(C)+\"/\"+str(gamma))\n",
        "\n",
        "# poly_param_list = []\n",
        "# for C in poly_C_range:\n",
        "#     for degree in degree_range:\n",
        "#         poly_param_list.append(('poly',C,degree))\n",
        "#         param_str_set.append(\"svm/poly/\"+str(C)+\"/\"+str(degree))\n",
        "        \n",
        "#svm_param_lst = rbf_param_list + poly_param_list\n",
        "\n",
        "param_str_set = []\n",
        "learning_rate = np.logspace(0.03, 0.3, num = 5) # default 0.1 \n",
        "max_depth = np.linspace(2, 6, num = 5, dtype=int) # default 3\n",
        "n_estimators = np.linspace(100,150, num = 4, dtype=int) # default 100\n",
        "\n",
        "xgb_param_lst = []\n",
        "for comb in itertools.product(learning_rate, max_depth, n_estimators):\n",
        "    xgb_param_lst.append(comb)\n",
        "    param_str_set.append(\"xgb/\"+str(comb[0])+\"/\"+str(comb[1])+\"/\"+str(comb[2]))\n",
        "\n",
        "\n",
        "# Initialize a dictionary to store the score obtained on each of the 4 validation sets for every parameter combination\n",
        "model_res_dict = {}\n",
        "for param_str in param_str_set:\n",
        "    model_res_dict[param_str] = []\n",
        "\n",
        "\n",
        "all_bins = [0,1,2,3,4]\n",
        "# This script runs considering bin 0 as the test set.\n",
        "test_bin = 3\n",
        "print(\"Fold: \"+str(test_bin))\n",
        "all_bins.remove(test_bin)\n",
        "print(\"train+val available bins:\")\n",
        "print(all_bins)\n",
        "\n",
        "train_marked_list = []\n",
        "param_list = []\n",
        "score_list = []\n",
        "# Nested inner CV to get results on 4 different validation sets and then choose the best model.\n",
        "for i in range(4):\n",
        "    print(\"Nested fold: \"+str(i))\n",
        "    avail_bins = []\n",
        "    # In each run choose a validation bin\n",
        "    val_bin = all_bins[i]\n",
        "    for num in all_bins:\n",
        "        if num != val_bin:\n",
        "            avail_bins.append(num)\n",
        "    \n",
        "    print(\"validation bin: \"+str(val_bin))\n",
        "    print(\"train available bins:\")\n",
        "    print(avail_bins)\n",
        "    \n",
        "    # get the genes associated with hormones for the trianing bins in this run\n",
        "    train_data = dict()\n",
        "    for hormone in hormone_gene_bins[str(val_bin)]:\n",
        "        train_data[hormone] = []\n",
        "    for bin_no in avail_bins:\n",
        "        for hormone in hormone_gene_bins[str(bin_no)].keys():\n",
        "            for gene in hormone_gene_bins[str(bin_no)][hormone]:\n",
        "                train_data[hormone].append(gene)\n",
        "    \n",
        "    # get SMOTE oversampled dataset\n",
        "    X_train, y_train, train_marked = get_oversampled_train_data(train_data,dup_genes)\n",
        "    train_marked_list.append(train_marked)\n",
        "    \n",
        "    #min_max_scaler = MinMaxScaler()\n",
        "    #min_max_scaler.fit(X_train)\n",
        "    #X_train = min_max_scaler.transform(X_train)\n",
        "    \n",
        "    # get the validation dataset from the validation bin\n",
        "    val_data = hormone_gene_bins[str(val_bin)]\n",
        "    neg_val_data = neg_hormone_gene_bins[str(val_bin)]\n",
        "    X_val_pos = transform_X_values(val_data,train_marked)\n",
        "    X_val_neg = transform_X_values(neg_val_data,train_marked)\n",
        "    X_val = np.concatenate([X_val_pos,X_val_neg])\n",
        "    y_val_pos = np.ones((X_val_pos.shape[0],), dtype=int)\n",
        "    y_val_neg = np.zeros((X_val_pos.shape[0],), dtype=int)\n",
        "    y_val = np.concatenate([y_val_pos, y_val_neg])\n",
        "    #X_val = min_max_scaler.transform(X_val)\n",
        "    \n",
        "    print(\"Validation shape\")\n",
        "    print(X_val.shape)\n",
        "    print(y_val.shape) \n",
        "\n",
        "    # execute the function to calculate the scores for multiple paramter combinations in parallel.\n",
        "    # n_jobs = -1 will use all the avaliable cores. set the cores as per availabilty\n",
        "    \n",
        "    # rf_results = [get_xgb_estimator_results(X_train,y_train,X_val,y_val,par) for par in xgb_param_lst]\n",
        "    # results = rf_results\n",
        "    \n",
        "    # # append the score obtained for each parameter combination in this run\n",
        "    # for pair in results:\n",
        "    #     model_res_dict[pair[0]].append(pair[1])\n",
        "\n",
        "    # print(\"Done with gridsearch\")\n",
        "\n",
        "    \n",
        "    # # append the score obtained for each parameter combination in this run\n",
        "    # for pair in results:\n",
        "    #     model_res_dict[pair[0]].append(pair[1])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N1nZm19bPbeY"
      },
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "classifier = LogisticRegression()\n",
        "classifier_type = \"logreg\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9B9GBu8D5it2"
      },
      "source": [
        "# for each parameter combination get the mean score across the 4 validation sets.\n",
        "model_scores = {}\n",
        "for param_comb in model_res_dict.keys():\n",
        "    model_scores[param_comb] = mean(model_res_dict[param_comb])\n",
        "\n",
        "with open('./BioEmbedS/output/bioembeds_model_scores_fold_'+str(test_bin)+'.json', 'w') as outfile:\n",
        "    json.dump(model_scores,outfile)\n",
        "outfile.close()\n",
        "\n",
        "# select the parameter combination having the best average score as our final classifier.\n",
        "best_combo = max(model_scores.items(), key=operator.itemgetter(1))\n",
        "best_score = best_combo[1]\n",
        "best_param = best_combo[0]\n",
        "classifier_type = best_param.split(\"/\")[0]\n",
        "print(\"best parameters: \"+best_param)\n",
        "print(\"best validation score: \"+str(best_score))\n",
        "\n",
        "if classifier_type == 'xgb':\n",
        "    param = {}\n",
        "\n",
        "    param['learning_rate'] = best_param.split(\"/\")[1]\n",
        "    param['max_depth'] = int(best_param.split(\"/\")[2])\n",
        "    param['n_estimators'] = int(best_param.split(\"/\")[3])\n",
        "\n",
        "    classifier = xgb.XGBClassifier()\n",
        "    classifier.set_params(**param)\n",
        "    print(classifier.get_params())\n",
        "\n",
        "else:\n",
        "    print(\"What the heck happened?\")\n",
        "    \n",
        "pickle.dump(classifier, open('./BioEmbedS/models/bioembeds_fold_'+str(test_bin)+'_model.sav', 'wb'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6kvXQuLWhocH"
      },
      "source": [
        "# get the genes associated with each hormone in the trianing+validation bins. \n",
        "# Oversample it using SMOTE and then fit our selected model to this dataset.\n",
        "train_val_data = dict()\n",
        "for hormone in hormone_gene_bins[str(test_bin)]:\n",
        "    train_val_data[hormone] = []\n",
        "for bin_no in all_bins:\n",
        "    for hormone in hormone_gene_bins[str(bin_no)].keys():\n",
        "        for gene in hormone_gene_bins[str(bin_no)][hormone]:\n",
        "            train_val_data[hormone].append(gene)\n",
        "            \n",
        "X_train_all, y_train_all, _train_marked = get_oversampled_train_data(train_val_data, dup_genes)\n",
        "\n",
        "#save the datasets\n",
        "np.save('./BioEmbedS/dataset/bioembeds_X_train_val_fold_'+str(test_bin)+'.npy',X_train_all)\n",
        "np.save('./BioEmbedS/dataset/bioembeds_y_train_val_fold_'+str(test_bin)+'.npy',y_train_all)\n",
        "with open('./BioEmbedS/dataset/train_val_marking_fold_'+str(test_bin)+'.json', 'w') as outfile:\n",
        "    json.dump(train_marked,outfile)\n",
        "outfile.close()\n",
        "\n",
        "classifier.fit(X_train_all,y_train_all)\n",
        "y_pred_train = classifier.predict(X_train_all)\n",
        "print(\"Training results: fold-\"+str(test_bin))\n",
        "print(confusion_matrix(y_train_all, y_pred_train))\n",
        "print(classification_report(y_train_all, y_pred_train))\n",
        "    \n",
        "# get the test data considering the test bin\n",
        "test_data = hormone_gene_bins[str(test_bin)]\n",
        "neg_test_data = neg_hormone_gene_bins[str(test_bin)]\n",
        "X_test_pos = transform_X_values(test_data,_train_marked)\n",
        "X_test_neg = transform_X_values(neg_test_data,_train_marked)\n",
        "X_test = np.concatenate([X_test_pos,X_test_neg])\n",
        "y_test_pos = np.ones((X_test_pos.shape[0],), dtype=int)\n",
        "y_test_neg = np.zeros((X_test_pos.shape[0],), dtype=int)\n",
        "y_test = np.concatenate([y_test_pos, y_test_neg])\n",
        "#X_test = min_max_scaler.transform(X_test)\n",
        "\n",
        "# get results on the test set\n",
        "y_pred_test = classifier.predict(X_test)\n",
        "if classifier_type == 'svm':\n",
        "    y_dec_score_test = classifier.decision_function(X_test)\n",
        "else:\n",
        "    y_dec_score_test = classifier.predict_proba(X_test)\n",
        "print(\"Testing results: fold-\"+str(test_bin))\n",
        "print(\"Kappa score: \"+str(cohen_kappa_score(y_test,y_pred_test)))\n",
        "print(confusion_matrix(y_test, y_pred_test))\n",
        "print(classification_report(y_test, y_pred_test))\n",
        "np.save('./BioEmbedS/output/y_fold_'+str(test_bin)+'.npy',y_test)\n",
        "np.save('./BioEmbedS/output/y_pred_fold_'+str(test_bin)+'.npy',y_pred_test)\n",
        "np.save('./BioEmbedS/output/y_dec_score_fold_'+str(test_bin)+'.npy',y_dec_score_test)\n",
        "print(\"ROC-AUC score: \"+str(roc_auc_score(y_test, y_dec_score_test[:, 1])))\n",
        "precision, recall, _ = precision_recall_curve(y_test, y_dec_score_test[:, 1])\n",
        "print(\"PR-AUC score: \"+str(auc(recall, precision)))\n",
        "get_binned_results(test_data, neg_test_data, _train_marked, classifier)\n",
        "print(\"\\n\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d_mhkn_eug4W"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}