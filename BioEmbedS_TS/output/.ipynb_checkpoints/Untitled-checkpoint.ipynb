{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import csv\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "import fasttext\n",
    "import math\n",
    "import operator\n",
    "import itertools\n",
    "import pickle\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import pyplot\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.combine import SMOTETomek\n",
    "from collections import Counter\n",
    "from sklearn.metrics import cohen_kappa_score\n",
    "from sklearn.metrics import roc_curve\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import auc\n",
    "from sklearn.metrics import precision_recall_curve\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from joblib import Parallel, delayed\n",
    "from statistics import mean\n",
    "model = fasttext.load_model(\"BioWordVec_PubMed_MIMICIII_d200.bin\")\n",
    "# get embedding for hormones having aliases by adding the original and alias word embedding.\n",
    "# Hormones having aliases have a \"/\" in between the two alias names\n",
    "alias_embeddings = dict()\n",
    "for hormone in hormone_src_tgt_genes.keys():\n",
    "    if \"/\" in hormone:\n",
    "        parts = hormone.split(\"/\")\n",
    "        w1 = model.get_word_vector(parts[0])\n",
    "        w2 = model.get_word_vector(parts[1])\n",
    "        alias_embeddings[hormone] = np.add(w1,w2)\n",
    "        \n",
    "# This is a list of genes that are associated with multiple hormones\n",
    "dup_genes = []\n",
    "with open('./BioEmbedS_TS/dataset/genes_associated_with_multiple_hormones.txt','r') as f:\n",
    "    for line in f:\n",
    "        dup_genes.append(line[:-1])\n",
    "        \n",
    "#a dict containing the hormone and the list of source and target genes associated with it in HGv1 database\n",
    "with open('./BioEmbedS_TS/dataset/hgv1_hormone_src_tgt_genes.json') as json_file:\n",
    "    hormone_src_tgt_genes = json.load(json_file)\n",
    "        \n",
    "#a dict containing the hormone and the list of genes associated with it in HGv1 database\n",
    "with open('./BioEmbedS_dummy/dataset/hgv1_hormone_genes.json') as json_file:\n",
    "    hormone_genes = json.load(json_file)\n",
    "\n",
    "    \n",
    "def transform_X_values(pairs):\n",
    "    embeddings = []\n",
    "    for item in pairs:\n",
    "        if \"/\" in item[0]:\n",
    "            np1 = alias_embeddings[item[0]]\n",
    "        else:\n",
    "            np1 = model.get_word_vector(item[0])\n",
    "        np2 = model.get_word_vector(item[1].lower())\n",
    "        embeddings.append(np.concatenate([np1,np2]))\n",
    "    return np.array(embeddings)\n",
    "\n",
    "def get_oversampled_train_data(src_train_data, tgt_train_data, dup_genes):\n",
    "    hor_map = dict()\n",
    "    train_marked = dict()\n",
    "    X_train_smote = []\n",
    "    y_train_smote = []\n",
    "    src_eligible_genes = dict()\n",
    "    src_duplicate_genes = dict()\n",
    "    tgt_eligible_genes = dict()\n",
    "    tgt_duplicate_genes = dict()\n",
    "    cnt = 1\n",
    "    # get the list of genes for each hormone after removing genes which are associated with multiple hormones\n",
    "    # (src_eligible genes and tgt_eligible_genes).\n",
    "    for hormone in src_train_data.keys():\n",
    "        src_eligible_genes[hormone] = []\n",
    "        src_duplicate_genes[hormone] = []\n",
    "        for gene in src_train_data[hormone]:\n",
    "            if gene in dup_genes:\n",
    "                src_duplicate_genes[hormone].append(gene)\n",
    "            else:\n",
    "                src_eligible_genes[hormone].append(gene)\n",
    "\n",
    "        tgt_eligible_genes[hormone] = []\n",
    "        tgt_duplicate_genes[hormone] = []\n",
    "        for gene in tgt_train_data[hormone]:\n",
    "            if gene in dup_genes:\n",
    "                tgt_duplicate_genes[hormone].append(gene)\n",
    "            else:\n",
    "                tgt_eligible_genes[hormone].append(gene)\n",
    "        \n",
    "        # Consider the source and target genes for every hormone as a different class for applying SMOTE\n",
    "        # Consider a hormone only if it has atleast 3 eligible source and 3 eligible target genes (constraint to apply SMOTE) \n",
    "        # and mark this gene.\n",
    "        if len(src_eligible_genes[hormone]) >= 3 and len(tgt_eligible_genes[hormone]) >= 3:\n",
    "            train_marked[hormone] = 1\n",
    "            hor_map[cnt] = hormone+'#source'\n",
    "            for gene in src_eligible_genes[hormone]:\n",
    "                X_train_smote.append(model.get_word_vector(gene))\n",
    "                y_train_smote.append(cnt)\n",
    "            cnt += 1\n",
    "            hor_map[cnt] = hormone+'#target'\n",
    "            for gene in tgt_eligible_genes[hormone]:\n",
    "                X_train_smote.append(model.get_word_vector(gene))\n",
    "                y_train_smote.append(cnt)\n",
    "            cnt += 1\n",
    "        else:\n",
    "            train_marked[hormone] = 0\n",
    "    \n",
    "    # transform dataset using smote-tomek\n",
    "    smote_strategy = SMOTETomek(smote=SMOTE(k_neighbors=2))\n",
    "    X_dataset_oversampled, y_dataset_oversampled = smote_strategy.fit_resample(np.array(X_train_smote), np.array(y_train_smote))\n",
    "    counter = Counter(y_dataset_oversampled)\n",
    "    print(counter)\n",
    "    oversampled_genes_per_hormone = dict()\n",
    "    X_train = []\n",
    "    \n",
    "    # get the oversampled embeddings for the source and the target sets\n",
    "    for hormone, embedding in zip(y_dataset_oversampled, X_dataset_oversampled):\n",
    "        if \"/\" in hor_map[hormone]:\n",
    "            w1 = alias_embeddings[hor_map[hormone].split(\"#\")[0]]\n",
    "        else:\n",
    "            w1 = model.get_word_vector(hor_map[hormone].split(\"#\")[0])\n",
    "\n",
    "        if \"source\" in hor_map[hormone]:\n",
    "            embedding = np.append(embedding,1)\n",
    "            X_train.append(np.concatenate([w1,embedding]))\n",
    "        if \"target\" in hor_map[hormone]:\n",
    "            embedding = np.append(embedding,0)\n",
    "            X_train.append(np.concatenate([w1,embedding]))\n",
    "\n",
    "        if hor_map[hormone] in oversampled_genes_per_hormone:\n",
    "            oversampled_genes_per_hormone[hor_map[hormone]].append(embedding)   \n",
    "        else:\n",
    "            oversampled_genes_per_hormone[hor_map[hormone]] = [embedding]\n",
    "    \n",
    "    # add back the genes associated with multiple hormones which were removed earlier.\n",
    "    for hormone in oversampled_genes_per_hormone.keys():\n",
    "        if \"/\" in hormone:\n",
    "            w1 = alias_embeddings[hormone.split(\"#\")[0]]\n",
    "        else:\n",
    "            w1 = model.get_word_vector(hormone.split(\"#\")[0])\n",
    "        for gene in src_duplicate_genes[hormone.split(\"#\")[0]]:\n",
    "            w2 = model.get_word_vector(gene)\n",
    "            w2 = np.append(w2,1)\n",
    "            X_train.append(np.concatenate([w1,w2]))\n",
    "        for gene in tgt_duplicate_genes[hormone.split(\"#\")[0]]:\n",
    "            w2 = model.get_word_vector(gene)\n",
    "            w2 = np.append(w2,0)\n",
    "            X_train.append(np.concatenate([w1,w2]))\n",
    "\n",
    "    X_train = np.array(X_train)\n",
    "    y_train = X_train[:,-1]\n",
    "    y_train = y_train.astype(np.int32)\n",
    "    X_train = np.delete(X_train,-1,axis=1)\n",
    "\n",
    "    print(\"Train shape\")\n",
    "    print(X_train.shape)\n",
    "    print(y_train.shape)\n",
    "    \n",
    "    return X_train, y_train, train_marked"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "hormone_src_genes = dict()\n",
    "hormone_tgt_genes = dict()\n",
    "\n",
    "for hormone in hormone_src_tgt_genes.keys():\n",
    "    hormone_src_genes[hormone] = []\n",
    "    hormone_tgt_genes[hormone] = []\n",
    "    for src_gene in hormone_src_tgt_genes[hormone]['source']:\n",
    "        hormone_src_genes[hormone].append(src_gene)\n",
    "    for tgt_gene in hormone_src_tgt_genes[hormone]['target']:\n",
    "        hormone_tgt_genes[hormone].append(tgt_gene)\n",
    "        \n",
    "X_train, y_train, _train_marked = get_oversampled_train_data(hormone_src_genes, hormone_tgt_genes, dup_genes)\n",
    "\n",
    "np.save('./BioEmbedS_TS/dataset/X_train_all_pairs.npy',X_train)\n",
    "np.save('./BioEmbedS_TS/dataset/y_train_all_pairs.npy',y_train)\n",
    "\n",
    "param = {}\n",
    "param['C'] = 1.0\n",
    "param['degree'] = 3\n",
    "param['kernel'] = 'poly'\n",
    "param['probability'] = True\n",
    "classifier = SVC()\n",
    "classifier.set_params(**param)\n",
    "classifier.fit(X_train,y_train)\n",
    "pickle.dump(classifier, open('./BioEmbedS_TS/models/bioembeds-ts_novel_predictions_model.sav', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name '_train_marked' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-27-4e9af2ae7fba>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mrow\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m         \u001b[0;32mif\u001b[0m \u001b[0mrow\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0m_train_marked\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpair\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m             \u001b[0mall_positive_preds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrow\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mrow\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name '_train_marked' is not defined"
     ]
    }
   ],
   "source": [
    "all_positive_preds = []\n",
    "with open('./BioEmbedS_dummy/all_novel_predictions.csv','rt')as f:\n",
    "    data = csv.reader(f)\n",
    "    next(data)\n",
    "    for row in data:\n",
    "        if row[4] == str(1) and _train_marked[row[0]] == 1:\n",
    "            all_positive_preds.append((row[0],row[1]))    \n",
    "            \n",
    "X_test_all_genes = transform_X_values(all_positive_preds)\n",
    "print(X_test_all_genes.shape)\n",
    "\n",
    "y_pred_all_genes = classifier.predict(X_test_all_genes)\n",
    "y_dec_func_all_genes = classifier.decision_function(X_test_all_genes)\n",
    "y_proba_all_genes = classifier.predict_proba(X_test_all_genes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_genes_results = []\n",
    "for pair, y_pred, y_dec, y_prob in zip(all_positive_preds, y_pred_all_genes, y_dec_func_all_genes, y_proba_all_genes):\n",
    "    if pair[1].lower() in hormone_genes[pair[0]]:\n",
    "        inHgv1 = \"Yes\"\n",
    "    else:\n",
    "        inHgv1 = \"No\"\n",
    "    if y_pred == 1:\n",
    "        pred = 'source'\n",
    "    else:\n",
    "        pred = 'target'\n",
    "    all_genes_results.append((pair[0], pair[1], inHgv1, pred, y_dec, y_prob[1]))\n",
    "    \n",
    "with open('./BioEmbedS_TS/bioembedsts_novel_predictions.csv', mode='w') as pred_file:\n",
    "    writer = csv.writer(pred_file, delimiter=',')\n",
    "    header = ['Hormone','Gene','Is pair in Hgv1?','Prediction','SVM score','SVM probability']\n",
    "    writer.writerow(header)\n",
    "    \n",
    "    for row in all_genes_results:\n",
    "        writer.writerow(row)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
